{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, iterations=5e5, break_threshold=1e-3, gamma=0.9):\n",
    "    optimal_values = { state: 0 for state in range(env.observation_space.n)}\n",
    "\n",
    "    for _ in range(int(iterations)):\n",
    "\n",
    "        max_threshold = -np.inf\n",
    "\n",
    "        for state in range(env.observation_space.n):\n",
    "            optimal_values_before = optimal_values[state]\n",
    "\n",
    "            max_value = -np.inf\n",
    "\n",
    "            for action in range(env.action_space.n):\n",
    "                crt_reward = None\n",
    "                crt_sum = 0\n",
    "\n",
    "                for prob, next_state, reward, _ in env.P[state][action]:\n",
    "                    if crt_reward is None:\n",
    "                        crt_reward = reward\n",
    "                    \n",
    "                    crt_sum += prob * optimal_values[next_state]\n",
    "\n",
    "                max_value = max(max_value, crt_reward + gamma * crt_sum)\n",
    "\n",
    "            optimal_values[state] = max_value\n",
    "\n",
    "            max_threshold = max(max_threshold, abs(optimal_values[state] - optimal_values_before))\n",
    "\n",
    "        if max_threshold < break_threshold:\n",
    "            break\n",
    "\n",
    "    return optimal_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(env, Q, state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        actions = list(env.P[state].keys()) \n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best_policy(env, Q):\n",
    "    policy = { state: None for state in range(env.observation_space.n) }\n",
    "\n",
    "    for state in range(env.observation_space.n):\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(env, Q, iterations):\n",
    "    epoch_rewards = []\n",
    "\n",
    "    policy = { state: None for state in range(env.observation_space.n) }\n",
    "\n",
    "    for state in range(env.observation_space.n):\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        env.reset()\n",
    "        state = 0\n",
    "        done = False\n",
    "\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        epoch_rewards.append(total_reward)\n",
    "        \n",
    "\n",
    "    return np.mean(epoch_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, optimal_values, gamma, epsilon, alpha, iterations):\n",
    "    env.reset()\n",
    "\n",
    "    rmses = []\n",
    "\n",
    "    # Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = None\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    if env_name == \"Taxi-v3\":\n",
    "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:\n",
    "        Q = np.random.uniform(low=-1, high=1, size=(env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        env.reset()\n",
    "        state = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = eps_greedy_policy(env, Q, state, epsilon)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "\n",
    "            Q[state, action] = (1- alpha) * Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]))\n",
    "            state = next_state\n",
    "\n",
    "        crt_values = { state: None for state in range(Q.shape[0]) }\n",
    "\n",
    "        for state in range(Q.shape[0]):\n",
    "            crt_values[state] = np.max(Q[state])\n",
    "        crt_rmse = np.sqrt(np.mean([(crt_values[state] - optimal_values[state])**2 for state in range(env.observation_space.n)]))\n",
    "        rmses.append(crt_rmse)\n",
    "\n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, optimal_values, gamma, epsilon, alpha, iterations):\n",
    "    env.reset()\n",
    "\n",
    "    rmses = []\n",
    "\n",
    "    # Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    Q = None\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    if env_name == \"Taxi-v3\":\n",
    "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:\n",
    "        Q = np.random.uniform(low=-1, high=1, size=(env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        env.reset()\n",
    "        state = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        action = eps_greedy_policy(env, Q, state, epsilon)\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_action = eps_greedy_policy(env, Q, next_state, epsilon)\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            Q[state, action] = (1- alpha) * Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        crt_values = { state: None for state in range(Q.shape[0]) }\n",
    "\n",
    "        for state in range(Q.shape[0]):\n",
    "            crt_values[state] = np.max(Q[state])\n",
    "\n",
    "        rmse = 0\n",
    "        for state in range(len(optimal_values)):\n",
    "            rmse += (optimal_values[state] - crt_values[state])**2\n",
    "\n",
    "        rmses.append(np.sqrt(rmse / len(optimal_values)))\n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nstep_sarsa(env, optimal_values, gamma, epsilon, alpha, n, iterations):\n",
    "    env.reset()\n",
    "\n",
    "    rmses = []\n",
    "\n",
    "    env_name = env.unwrapped.spec.id\n",
    "    Q = None\n",
    "    if env_name == \"Taxi-v3\":\n",
    "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:\n",
    "        Q = np.random.uniform(low=-1, high=1, size=(env.observation_space.n, env.action_space.n))\n",
    "    G = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        env.reset()\n",
    "        state = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        T = sys.maxsize\n",
    "        t = 0\n",
    "\n",
    "\n",
    "        action = eps_greedy_policy(env, Q, state, epsilon)\n",
    "        state_action_rewards = [(state, action, 0)]\n",
    "        while t < T - 1:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if done:\n",
    "                T = t + 1\n",
    "            else:\n",
    "                action = eps_greedy_policy(env, Q, next_state, epsilon)\n",
    "\n",
    "            state_action_rewards.append((next_state, action, reward))\n",
    "\n",
    "            tau = t - n + 1\n",
    "\n",
    "            # print(len(state_action_rewards))\n",
    "\n",
    "            if tau >= 0:\n",
    "                # print(\"tau: \", tau)\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T) + 1):\n",
    "                    # print(\"i: \", i)\n",
    "                    G += np.power(gamma, i - tau - 1) * state_action_rewards[i][2]\n",
    "\n",
    "                if tau + n < T:\n",
    "                    G += np.power(gamma, n) * Q[state_action_rewards[tau + n][0], state_action_rewards[tau + n][1]]\n",
    "\n",
    "                state, action = state_action_rewards[tau][0], state_action_rewards[tau][1]\n",
    "                Q[state, action] = (1- alpha) * Q[state, action] + alpha * G\n",
    "            \n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        crt_values = { state: None for state in range(Q.shape[0]) }\n",
    "\n",
    "        for state in range(Q.shape[0]):\n",
    "            crt_values[state] = np.max(Q[state])\n",
    "\n",
    "        rmse = 0\n",
    "        for state in range(len(optimal_values)):\n",
    "            rmse += (optimal_values[state] - crt_values[state])**2\n",
    "\n",
    "        rmses.append(np.sqrt(rmse / len(optimal_values)))\n",
    "\n",
    "    return rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha:  0.0\n",
      "iter:  0\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  1\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  2\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  3\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  4\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  5\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  6\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  7\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  8\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  9\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  10\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  11\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  12\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  13\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  14\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  15\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  16\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  17\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  18\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  19\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "For alpha:  0.2\n",
      "iter:  0\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  1\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  2\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  3\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  4\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  5\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  6\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  7\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  8\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  9\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  10\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  11\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  12\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  13\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  14\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  15\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  16\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  17\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  18\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  19\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "For alpha:  0.4\n",
      "iter:  0\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  1\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  2\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  3\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  4\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  5\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  6\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  7\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  8\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  9\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  10\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  11\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  12\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  13\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n",
      "8 -step sarsa\n",
      "16 -step sarsa\n",
      "iter:  14\n",
      "2 -step sarsa\n",
      "4 -step sarsa\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m ns:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(n,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-step sarsa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m         nstep_rmse[ns\u001b[38;5;241m.\u001b[39mindex(n)]\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnstep_sarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     37\u001b[0m qs_rmse_env\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(qs_rmse))\n\u001b[1;32m     38\u001b[0m sarsa_rmse_env\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(sarsa_rmse))\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mnstep_sarsa\u001b[0;34m(env, optimal_values, gamma, epsilon, alpha, n, iterations)\u001b[0m\n\u001b[1;32m     46\u001b[0m     G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(gamma, i \u001b[38;5;241m-\u001b[39m tau \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m state_action_rewards[i][\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tau \u001b[38;5;241m+\u001b[39m n \u001b[38;5;241m<\u001b[39m T:\n\u001b[0;32m---> 49\u001b[0m     G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m Q[state_action_rewards[tau \u001b[38;5;241m+\u001b[39m n][\u001b[38;5;241m0\u001b[39m], state_action_rewards[tau \u001b[38;5;241m+\u001b[39m n][\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     51\u001b[0m state, action \u001b[38;5;241m=\u001b[39m state_action_rewards[tau][\u001b[38;5;241m0\u001b[39m], state_action_rewards[tau][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     52\u001b[0m Q[state, action] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m Q[state, action] \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m G\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import random\n",
    "import pickle    \n",
    "    \n",
    "random.seed(123)\n",
    "\n",
    "alphas = np.arange(0, 1.2, 0.20)\n",
    "ns = [2, 4, 8, 16]\n",
    "\n",
    "env_name = \"Taxi-v3\"\n",
    "env = gym.make(env_name)\n",
    "optimal_v = value_iteration(env)\n",
    "\n",
    "qs_rmse_env = []\n",
    "sarsa_rmse_env = []\n",
    "nstep_rmse_env = [[] for _ in range(len(ns))]\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"For alpha: \", alpha)\n",
    "    qs_rmse = []\n",
    "    sarsa_rmse = []\n",
    "    nstep_rmse = [[] for _ in range(len(ns))]\n",
    "\n",
    "    for iteration in range(20):\n",
    "        print(\"iter: \", iteration)\n",
    "\n",
    "        qs_rmse.append(np.mean(q_learning(env, optimal_v, 0.9, 0.1, alpha, 2000)))\n",
    "        sarsa_rmse.append(np.mean(sarsa(env, optimal_v, 0.9, 0.1, alpha, 2000)))\n",
    "\n",
    "        for n in ns:\n",
    "            print(n,\"-step sarsa\")\n",
    "            nstep_rmse[ns.index(n)].append(np.mean(nstep_sarsa(env, optimal_v, 0.9, 0.1, alpha, n, 2000)))\n",
    "\n",
    "    qs_rmse_env.append(np.mean(qs_rmse))\n",
    "    sarsa_rmse_env.append(np.mean(sarsa_rmse))\n",
    "    \n",
    "    for i in range(len(ns)):\n",
    "        nstep_rmse_env[i].append(np.mean(nstep_rmse[i]))\n",
    "\n",
    "plt.plot(alphas, qs_rmse_env, label=\"q-learning\")\n",
    "plt.plot(alphas, sarsa_rmse_env, label=\"sarsa\")\n",
    "\n",
    "for i in range(len(ns)):\n",
    "    plt.plot(alphas, nstep_rmse_env[i], label=\"{}-step sarsa\".format(ns[i]))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(alphas)\n",
    "plt.title(env_name)\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Taxi-v3\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
